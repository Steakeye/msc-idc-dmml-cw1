---
title: "WhiteWine"
output: html_document
---


```{r setup, include=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r, echo=FALSE, include=FALSE, message=FALSE}
mirrorUrl = "http://cran.ma.imperial.ac.uk"

# Install and load all packages up-front!
if(!require(readxl)) install.packages("readxl", repos = mirrorUrl)
if(!require(cluster)) install.packages("cluster", repos = mirrorUrl)
if(!require(NbClust)) install.packages("NbClust", repos = mirrorUrl)
if(!require(fpc)) install.packages("fpc", repos = mirrorUrl, dependencies = TRUE)
if(!require(flexclust)) install.packages("flexclust", repos = mirrorUrl, dependencies = TRUE)
#
library("readxl")
library("fpc")
library("flexclust")

set.seed(1234)
```

<section>

#Question 1: White Wine clustering

##Starting off

>You need to conduct the k-means clustering analysis of the white wine sheet. Find the ideal number of clusters (please justify your answer). Choose the best two possible numbers of clusters and perform the k-means algorithm for both candidates. Validate which clustering test is more accurate. For the winning test, get the mean of the each attribute of each group. Before conducting the k-means, please investigate if you need to add in your code any pre-processing task (justify your answer). Write a code in R Studio to address all the above issues. In your report, check the consistency of those produced clusters, with information obtained from column 12.

In the White Win dataset provided, column 12 is labelled Quality; this is a qualitative value assigned by a human through the subjective means of tasting. Essentially, by try to cluster against all variables apart from Quality and then comparing against this variable, we are trying to look for some correlation between all the variables in combination and the subjective quality of wine.

<!--
1st Objective (partitioning clustering)
justify it by showing all necessary steps/methods, 8
Find the mean of each attribute for the winner cluster, 5
Check for any pre-processing tasks 5
-->

Firstly we need to load the data...

```{r}
#going to import the Excel spreadsheet WhiteWine dataset
wine.raw <- read_excel("../data/Whitewine.xlsx")
```

Here's a glance at the dataset

```{r}
head(wine.raw)
str(wine.raw)
```

We want to scale the data to allow all attributes to be compared more easily. First of all let's split our data so we have two tables, one with all the attributes of wine and the other just with the humanly perceived quality. 

```{r}
wine.all_but_q <- wine.raw[1:11]
wine.q <- wine.raw$quality

#Wine properties
str(wine.all_but_q)

#Wine quality values
str(wine.q)
```

Now we scale the data
```{r}
wine.scaled <- as.data.frame(scale(wine.all_but_q))

#Summary of scaled wine data
summary(wine.scaled)
```

```{r}
boxplot(wine.scaled, main="Looking at the data graphically", xlab="Wine Attributes", ylab="Scaled values") 
```

We can see from these box-plots that some attributes seem to have some clear outliers that would suggest erroneous data and not just natural extremes. As such, we can decide that it's worth cleansing the data a little by removing these outliers from the dataset. For example, Alcohol on the most right column seems to have very clear boundaries as we'd expect from wine; when that is compared with some other attributes, they seem to tell a different story: Chlorides seems to have a lot of values that are in the upper quartile, and a large distance between min and max values but when you look it you can see there's a gradient that suggests a normal distribution; in contrast to this, the columns Residual Sugar, Free Sulfur Dioxide and Density all seem to not only have relatively large min and max distances but there seem to be uppermost values that with nearest neighbour values that are a relatively large distance away.

<!--
https://www.researchgate.net/post/Should_outliers_be_removed_before_or_after_data_transformation
-->

Below are density line graphs to demonstrate the difference between attributes that don't seem to have outliers compared to those that do.

```{r}
plot(density(wine.scaled$`alcohol`))
plot(density(wine.scaled$`chlorides`))
plot(density(wine.scaled$`free sulfur dioxide`))
plot(density(wine.scaled$`density`))
```

In order to work out which attributes should be considered to have valid outliers, I've gone with a heuristic approach, choosing to look at the distance between the uppermost outliers for each attribute and it's nearest neighbour.

```{r}
#Create a list to populate with our tail neighbour distances
tail_deltas <- c()

for (attrib in wine.scaled) {
 #get the last two values
 data_tails <- tail(sort(attrib),2)
 #push the delta on to our list 
 tail_deltas <- c(tail_deltas, diff(data_tails))
}

#grab out attribute keys to include in our new table/frame
attributes <- names(wine.scaled)

#make a new dataframe from 
dataframe <- data.frame(attributes = attributes, tail_neighbour_d=tail_deltas)

#get the order for the nearest neighbour starting with the greatest distance and descending
neighbour_order <- order(dataframe$tail_neighbour_d, decreasing=TRUE)

#now apply the order to the frame
sorted_attributes_by_neighbour_d <- dataframe[ neighbour_order, ]
sorted_attributes_by_neighbour_d
```

Given the findings, I think we can just consider the top five attributes in the above list as ones to cleanse for outliers. A lot of sources online warn against arbitrarily getting rid of outliers because it might be the case that valid information is being lost when what you really want to account for is bad data.

To clarify, the attributes to be processed are:
- density			
-	free sulfur dioxide			
-	residual sugar		
-	citric acid 		
-	fixed acidity

Boxplot has an outlier property that we can use to collect values that we might want to remove, so this is the one option we will look at for cleansing data.

```{r}
wine.scaled_cleansed_bp <- wine.scaled[ !(wine.scaled$density %in% boxplot(wine.scaled$density, plot=FALSE)$out), ]
wine.scaled_cleansed_bp <- wine.scaled_cleansed_bp[ !(wine.scaled_cleansed_bp$`free sulfur dioxide` %in% boxplot(wine.scaled$`free sulfur dioxide`, plot=FALSE)$out), ]
wine.scaled_cleansed_bp <- wine.scaled_cleansed_bp[ !(wine.scaled_cleansed_bp$`residual sugar` %in% boxplot(wine.scaled_cleansed_bp$`residual sugar`, plot=FALSE)$out), ]
wine.scaled_cleansed_bp <- wine.scaled_cleansed_bp[ !(wine.scaled_cleansed_bp$`citric acid` %in% boxplot(wine.scaled_cleansed_bp$`citric acid`, plot=FALSE)$out), ]
wine.scaled_cleansed_bp <- wine.scaled_cleansed_bp[ !(wine.scaled_cleansed_bp$`fixed acidity` %in% boxplot(wine.scaled_cleansed_bp$`fixed acidity`, plot=FALSE)$out), ]

boxplot(wine.scaled_cleansed_bp, main="Looking at the cleansed data graphically", xlab="Wine Attributes", ylab="Scaled values") 

```

While this new set of data is now has no values beyond the outermost quartile ranges, this is arguably too harsh a treatment. An alternative option is to arbitrarily work with the interquartile ranges; what  have done is to tweak the multiplier of the interquartile range until it successfully meant that only the most extreme outliers were discard. In the end a value 5 times that of the IQR worked well to pick off only values at the very tips of the tails.

```{r, message=FALSE}

#Get the top 5 variables with the highest outlier distance
worst_outliers <- head(sorted_attributes_by_neighbour_d$attributes, n=5)

wine.scaled_cleansed_iqr <- wine.scaled

# Create a variable to store the row id's to be removed
iqr_outliers <- c()
quartile_multiplier = 5

# Loop through the list of columns you specified
for(i in worst_outliers){

 # Get the Min/Max values
 max <- quantile(wine.scaled_cleansed_iqr[,i],0.75, na.rm=FALSE) + (IQR(wine.scaled_cleansed_iqr[,i], na.rm=FALSE) * quartile_multiplier )
 min <- quantile(wine.scaled_cleansed_iqr[,i],0.25, na.rm=FALSE) - (IQR(wine.scaled_cleansed_iqr[,i], na.rm=FALSE) * quartile_multiplier )
 
 # Get the id's using which
 idx <- which(wine.scaled_cleansed_iqr[,i] < min | wine.scaled_cleansed_iqr[,i] > max)
 
 # Output the number of outliers in each variable
 #print(paste(i, length(idx), sep=' - removing: '))
 
 # Append the outliers list
 iqr_outliers <- c(iqr_outliers, idx) 
}

# Sorting outliers
iqr_outliers <- sort(iqr_outliers)

# Remove the outliers
wine.scaled_cleansed_iqr <- wine.scaled_cleansed_iqr[-iqr_outliers,]

boxplot(wine.scaled_cleansed_iqr, main="Looking at the IQR cleansed data graphically", xlab="Wine Attributes", ylab="Scaled values") 

```

Now that the data looks a lot cleaner, it's time to start working with the data to try and find the best clustering. To begin with, nbclust will be used to see if that produces anything useful.

```{r}
number_of_clusters <- NbClust(wine.scaled_cleansed_iqr,
                min.nc=2, max.nc=15,
                method="kmeans")
```

The following table displays the results recommending potential values for k
```{r}
table(number_of_clusters$Best.n[1,])
```

The bar chart more easily conveys this.
```{r}
barplot(table(number_of_clusters$Best.n[1,]), 
       xlab="Number of Clusters",
       ylab="Number of Criteria",
       main="Number of Clusters Chosen by 30 Criteria")
```

From the bar graph above we can see that there seems to be an clear leader in terms of suggested number of clusters, being k = 2. There are however other values that should be explored to see how they compare: 3, 5 and 14. To confirm that the accuracy of this result in terms of the best contender for, we can plot the sum of square errors and looks for a pronounced bend in the graph. Where the most pronounced bend is, this is a contender for the value for k.

```{r}
sse_list <- 0
for (i in 1:15){
 sse_list[i] <- sum(kmeans(wine.scaled_cleansed_iqr, centers=i)$withinss)
}

plot(1:15,
 sse_list,
 type="b",
 xlab="Number of Clusters",
 ylab="Within groups sum of squares")
```

The histogram for the Sum of Square Errors partially backs up the results of nbclust seeing as there is 'elbow' on the line at 2 on the Number of Clusters. Having said that, the kink between 5 and 7 suggest that this range should also be tested for k. 

```{r}

#If we're going to run tests on the k-means against the data we need to remove the outliers from our quality column too
wine.q_cleansed <- wine.q[-iqr_outliers]

fit.km2 <- kmeans(wine.scaled_cleansed_iqr, 2)
fit.km3 <- kmeans(wine.scaled_cleansed_iqr, 3)
fit.km4 <- kmeans(wine.scaled_cleansed_iqr, 4)
fit.km5 <- kmeans(wine.scaled_cleansed_iqr, 5)
fit.km6 <- kmeans(wine.scaled_cleansed_iqr, 6)
fit.km7 <- kmeans(wine.scaled_cleansed_iqr, 7)
fit.km11 <- kmeans(wine.scaled_cleansed_iqr, 11)
fit.km14 <- kmeans(wine.scaled_cleansed_iqr, 14)

plotcluster(wine.scaled_cleansed_iqr, fit.km2$cluster)
plotcluster(wine.scaled_cleansed_iqr, fit.km3$cluster)
plotcluster(wine.scaled_cleansed_iqr, fit.km4$cluster)
plotcluster(wine.scaled_cleansed_iqr, fit.km5$cluster)
plotcluster(wine.scaled_cleansed_iqr, fit.km6$cluster)
plotcluster(wine.scaled_cleansed_iqr, fit.km7$cluster)
plotcluster(wine.scaled_cleansed_iqr, fit.km11$cluster)
plotcluster(wine.scaled_cleansed_iqr, fit.km14$cluster)
```


## Mapping/Fitting the clusters to the data

Now that we have experimented with various different values for k, when applying k-means clustering to the wine data, it's now time to see if it can be fit to the data that delivers anything obviously meaningful with regards to wine quality. While the strongest cluster option appears to be 2, I thought it was worth looking at how to map the data against quality by looking at the quality values as though they were factors; so, as you can see in the following table, there are only 7 unique values out of 10 possible scores for quality, meaning that trying to fit the data is actually easiest against 7 clusters, one per quality value.

To comapre the clusters to the quality scores we use a Confusion matrix to see where the values lie within those 2 sets of data. Further to that, to evaluate the confusion matrix mathematically, we will apply the Rand Index method, which Wikipedia (https://en.wikipedia.org/wiki/Cluster_analysis#External_evaluation) describes thusly:

>The Rand index computes how similar the clusters (returned by the clustering algorithm) are to the benchmark classifications. One can also view the Rand index as a measure of the percentage of correct decisions made by the algorithm. It can be computed using the following formula RI={\frac {TP+TN}{TP+FP+FN+TN}}

```{r}
wine.q_table <- table(wine.q_cleansed)
wine.q_table
barplot(wine.q_table,
       xlab="Quality values",
       ylab="Frequency",
       main="Distribution of wines across quality values")
```
```{r}
confuseTable.km7 <- table(wine.q_cleansed, fit.km7$cluster)

names(dimnames(confuseTable.km7)) <- list("Quality", "Clusters")

confuseTable.km7

randIndex(confuseTable.km7)

```
### Poor results

Given this low value of 0.03410079, is so far from the ideal, and as can be seen from the matrix, there seems to be a spread across all clusters, we can surmise that either the White Wine dataset was not cleansed thoroughly enough or that k-means clustering i=simply isn't an effective way of determining quality.

In order to be sure that it is indeed the methodology that is unsuitable rather than the data being insufficiently processed, looking at a more severe form of data cleansing may prove insightful; to that end, removing all boxplot outliers across all variables and running the whole process again is worth it just to see if the results are more conclusive.

```{r}
wine.properties <- names(wine.all_but_q)
# 
wine.scaled_cleansed_bp_all <- wine.scaled

wine.scaled_cleansed_bp_all <- wine.scaled_cleansed_bp_all[ !(wine.scaled_cleansed_bp_all$density %in% boxplot(wine.scaled_cleansed_bp_all$density, plot=FALSE)$out), ]
wine.scaled_cleansed_bp_all <- wine.scaled_cleansed_bp_all[ !(wine.scaled_cleansed_bp_all$`free sulfur dioxide` %in% boxplot(wine.scaled_cleansed_bp_all$`free sulfur dioxide`, plot=FALSE)$out), ]
wine.scaled_cleansed_bp_all <- wine.scaled_cleansed_bp_all[ !(wine.scaled_cleansed_bp_all$`residual sugar` %in% boxplot(wine.scaled_cleansed_bp_all$`residual sugar`, plot=FALSE)$out), ]
wine.scaled_cleansed_bp_all <- wine.scaled_cleansed_bp_all[ !(wine.scaled_cleansed_bp_all$`citric acid` %in% boxplot(wine.scaled_cleansed_bp_all$`citric acid`, plot=FALSE)$out), ]
wine.scaled_cleansed_bp_all <- wine.scaled_cleansed_bp_all[ !(wine.scaled_cleansed_bp_all$`fixed acidity` %in% boxplot(wine.scaled_cleansed_bp_all$`fixed acidity`, plot=FALSE)$out), ]
wine.scaled_cleansed_bp_all <- wine.scaled_cleansed_bp_all[ !(wine.scaled_cleansed_bp_all$`volatile acidity` %in% boxplot(wine.scaled_cleansed_bp_all$`volatile acidity`, plot=FALSE)$out), ]
wine.scaled_cleansed_bp_all <- wine.scaled_cleansed_bp_all[ !(wine.scaled_cleansed_bp_all$chlorides %in% boxplot(wine.scaled_cleansed_bp_all$chlorides, plot=FALSE)$out), ]
wine.scaled_cleansed_bp_all <- wine.scaled_cleansed_bp_all[ !(wine.scaled_cleansed_bp_all$`total sulfur dioxide` %in% boxplot(wine.scaled_cleansed_bp_all$`total sulfur dioxide`, plot=FALSE)$out), ]
wine.scaled_cleansed_bp_all <- wine.scaled_cleansed_bp_all[ !(wine.scaled_cleansed_bp_all$pH %in% boxplot(wine.scaled_cleansed_bp_all$pH, plot=FALSE)$out), ]
wine.scaled_cleansed_bp_all <- wine.scaled_cleansed_bp_all[ !(wine.scaled_cleansed_bp_all$sulphates %in% boxplot(wine.scaled_cleansed_bp_all$sulphates, plot=FALSE)$out), ]
wine.scaled_cleansed_bp_all <- wine.scaled_cleansed_bp_all[ !(wine.scaled_cleansed_bp_all$alcohol %in% boxplot(wine.scaled_cleansed_bp_all$alcohol, plot=FALSE)$out), ]

# for (prop in wine.properties) {
#   wine.scaled_cleansed_bp_all <- wine.scaled_cleansed_bp_all[ !(wine.scaled_cleansed_bp_all[prop] %in% boxplot(wine.scaled_cleansed_bp_all[prop], plot=FALSE)$out), ]
# }

boxplot(wine.scaled_cleansed_bp_all, main="Boxplot all outliers cleansed", xlab="Wine Attributes", ylab="Scaled values")
```

```{r NbClust}
number_of_clusters_severe_cleanse <- NbClust(wine.scaled_cleansed_bp_all,
                min.nc=2, max.nc=15,
                method="kmeans")
```


The following table displays the results recommending potential values for k
```{r}
table(number_of_clusters_severe_cleanse$Best.n[1,])
```

The bar chart more easily conveys this.
```{r}
barplot(table(number_of_clusters_severe_cleanse$Best.n[1,]), 
       xlab="Number of Clusters",
       ylab="Number of Criteria",
       main="Number of Clusters Chosen by 30 Criteria")
```

```{r}
sse_list <- 0
for (i in 1:15){
 sse_list[i] <- sum(kmeans(wine.scaled_cleansed_bp_all, centers=i)$withinss)
}

plot(1:15,
 sse_list,
 type="b",
 xlab="Number of Clusters",
 ylab="Within groups sum of squares")
```

```{r}

#If we're going to run tests on the k-means against the severely cleansed data we need to remove the outliers from our quality column too
bp_server_outliers <- unique(unlist(mapply(function(x, y) sapply(setdiff(x, y), function(d) which(x==d)), wine.scaled, wine.scaled_cleansed_bp_all)))

wine.q_cleansed_severe <- wine.q[-bp_server_outliers]

fit_severe.km2 <- kmeans(wine.scaled_cleansed_bp_all, 2)
fit_severe.km3 <- kmeans(wine.scaled_cleansed_bp_all, 3)
fit_severe.km4 <- kmeans(wine.scaled_cleansed_bp_all, 4)
fit_severe.km5 <- kmeans(wine.scaled_cleansed_bp_all, 5)
fit_severe.km6 <- kmeans(wine.scaled_cleansed_bp_all, 6)
fit_severe.km7 <- kmeans(wine.scaled_cleansed_bp_all, 7)
fit_severe.km11 <- kmeans(wine.scaled_cleansed_bp_all, 11)
fit_severe.km14 <- kmeans(wine.scaled_cleansed_bp_all, 14)

plotcluster(wine.scaled_cleansed_bp_all, fit_severe.km2$cluster)
plotcluster(wine.scaled_cleansed_bp_all, fit_severe.km3$cluster)
plotcluster(wine.scaled_cleansed_bp_all, fit_severe.km4$cluster)
plotcluster(wine.scaled_cleansed_bp_all, fit_severe.km5$cluster)
plotcluster(wine.scaled_cleansed_bp_all, fit_severe.km6$cluster)
plotcluster(wine.scaled_cleansed_bp_all, fit_severe.km7$cluster)
plotcluster(wine.scaled_cleansed_bp_all, fit_severe.km11$cluster)
plotcluster(wine.scaled_cleansed_bp_all, fit_severe.km14$cluster)
```

```{r}
confuseTable_severe.km7 <- table(wine.q_cleansed_severe, fit_severe.km7$cluster)

names(dimnames(confuseTable_severe.km7)) <- list("Quality", "Clusters")

confuseTable_severe.km7

randIndex(confuseTable_severe.km7)

#this looks rubbish, let's try creating a factor of category, poor, okay, good and see if that an be matched up to a cluster of 3 (https://www.r-bloggers.com/from-continuous-to-categorical/)
```

##Results so far

Given that the poor value of 0.03410079 for the dataset that was only lightly 'pruned' is actually closer to 1  than the 0.02536692 of the last result, the previous dataset actually led to a better set of clusters to act as predictors of Quality.

### Alternative clusters

It would appear that the less intensive data cleansing was more appropriate if the ARI value is anything to go by. To refresh what the confusion matrix for that looked like, it is repeated below:

```{r}

confuseTable.km7

randIndex(confuseTable.km7)

#this looks rubbish, let's try creating a factor of category, poor, okay, good and see if that an be matched up to a cluster of 3 (https://www.r-bloggers.com/from-continuous-to-categorical/)
```

If we look at this more closely, we can see that while there are wines of various qualities spread across all clusters, there are some clusters are weighted in favour of higher quality wines or the middle range. From this observation it can be posited that some more meaningful fitting might be found between factors of quality, "Good", "Mediocre" and "Bad". So one final experiment before drawing to a conclusion is to try to fit the data against 3 clusters.

## Creating 3 quality factors & attempting one last fit.

```{r}
wine.q_cleansed_f3 <- cut(wine.q_cleansed, 3, labels = c("bad", "mediocre", "good"))

confuseTable.km3 <- table(wine.q_cleansed, fit.km3$cluster)
names(dimnames(confuseTable.km3)) <- list("Quality", "Clusters")

confuseTable.km3_f3 <- table(wine.q_cleansed_f3, fit.km3$cluster)
names(dimnames(confuseTable.km3_f3)) <- list("Quality", "Clusters")

confuseTable.km3
randIndex(confuseTable.km3)

confuseTable.km3_f3
randIndex(confuseTable.km3_f3)
```

The results of these other attempts to fit the data against 3 clusters has not yielded (significantly) better results. The very last thing is to see how the suggestion by NbClust works out.

### Fitting to NbClust suggested k = 2

```{r}
wine.q_cleansed_f2 <- cut(wine.q_cleansed, 2, labels = c("bad", "good"))

confuseTable.km2 <- table(wine.q_cleansed, fit.km2$cluster)
names(dimnames(confuseTable.km2)) <- list("Quality", "Clusters")

confuseTable.km2_f2 <- table(wine.q_cleansed_f2, fit.km3$cluster)
names(dimnames(confuseTable.km2_f2)) <- list("Quality", "Clusters")

confuseTable.km2
randIndex(confuseTable.km2)

confuseTable.km2_f2
randIndex(confuseTable.km2_f2)
```

## Writing up for the best results

According to the ARI values the highest being 0.03450436, for 3 clusters with 7 unique quality values, this would be the most successful k-means clustering of those explored, though only marginally. So before we reach out conclusion it's necessary to display the characteristics of this particular set of clusters for k = 3.

```{r}
#fit.km3

#K-means clustering with 3 clusters of sizes:
fit.km3$size

#Cluster means:
fit.km3$centers
```

## Conclusion

By the looks of it, this use of k-means is simply not appropriate against this set of data; it would seem that either using Principal Component Analysis or applying a method for being selective about which variables are used when looking for certain trends would be required in order to reduce the noise that comes from having so many dimensions. One tool in Data Science that could have proved beneficial for this would have been Exploratory Data Analysis; finding relationships between different variables and those connections to Quality might have led to a better understanding of factors affecting wine qualities, resulting some idea of how to select only certain variable to apply k-means to. Additionally, would consider initially testing against a smaller sample of data next time before investing so many CPU cycles to this task! 

</section>
