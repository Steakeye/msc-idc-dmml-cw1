---
title: "WhiteWine"
output: html_document
---


```{r setup, include=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r, echo=FALSE, include=FALSE}
mirrorUrl = "http://cran.ma.imperial.ac.uk"

# Install and load all packages up-front!
if(!require(readxl)) install.packages("readxl",repos = mirrorUrl)
if(!require(cluster)) install.packages("cluster",repos = mirrorUrl)
if(!require(NbClust)) install.packages("NbClust",repos = mirrorUrl)

library("readxl")
```
#Question 1: White Wine clustering

```{r test-main, child = 'test.rmd'}
```

##Starting off

>You need to conduct the k-means clustering analysis of the white wine sheet. Find the ideal number of clusters (please justify your answer). Choose the best two possible numbers of clusters and perform the k-means algorithm for both candidates. Validate which clustering test is more accurate. For the winning test, get the mean of the each attribute of each group. Before conducting the k-means, please investigate if you need to add in your code any pre-processing task (justify your answer). Write a code in R Studio to address all the above issues. In your report, check the consistency of those produced clusters, with information obtained from column 12.

<!--
1st Objective (partitioning clustering)
• Find the ideal number of clusters – justify it by showing all necessary steps/methods, 8
• K-means with the best two clusters, 8
• Find the mean of each attribute for the winner cluster, 5
• Check consistency of your results against column 12, 4
• Check for any pre-processing tasks 5
-->

Firstly we need to load the data...

```{r}
#going to import the Excel spreadsheet WhiteWine dataset
# xlsx files
wine.raw <- read_excel("../data/Whitewine.xlsx")
head(wine.raw)
```

Here's a a glance at the datset

```{r}
str(wine.raw)

#Normalise the WhiteWine dataset from Excel import

#data.train <- scale(my_data[-1])
#summary(data.train)
#nc <- NbClust(data.train,
#min.nc=2, max.nc=15,
#method="kmeans") #runtime hangs at this point - needs optimising
#barplot(table(nc$Best.n[1,]),
#xlab="Numer of Clusters",
#ylab="Number of Criteria",
#main="Number of Clusters Chosen by 26 Criteria")
```

We want to scale the data to allow all attributes to be compared more easily. First of all let's split our data so we have two tables, one with all the attributes of wine and the other just with the humanly percieved quality 
```{r}
wine.all_but_q <- wine.raw[1:11]
wine.q <- wine.raw$quality

str(wine.all_but_q)
str(wine.q)
#wine.scaled <- scale(wine.raw)
```

Now we scale the data
```{r}
wine.scaled <- as.data.frame(scale(wine.all_but_q))

summary(wine.scaled)
```

```{r}
boxplot(wine.scaled, main="Looking at the data graphically", xlab="Wine Attributes", ylab="Scalled values") 
```

We can see from these box-plots that some attributes seem to have some clear outliers that would suggest erroneous data and not just natural extremes. As such, we can decide that it's worth cleansing the data a little by removing these outliers from the dataset. For example, Alcohol on the most right column seesms to have very clear boundaries as we'd expect from wine; when that is compared with some other attributes, they seem to tell a different story: Chlorides seems to have a lot of values that are in the upper quartile, and a large distance etween min and max values but when you look it you can see there's a gradient that suggests a normal distribution; in contrast to this, the columns Residual Sugar, Free Sulfur Dioxide and Density all seem to not only have relatively large min and max distances but there seem to be uppermost values that with nearest neighbour values that are a relatively large distance away.

Below are density line graphs to demonstrate the difference between attributes that don't seem to have outliers compared to those that do.

```{r}
plot(density(wine.scaled$`alcohol`))
plot(density(wine.scaled$`chlorides`))
plot(density(wine.scaled$`free sulfur dioxide`))
plot(density(wine.scaled$`density`))
```

In order to work out which attributes should be considered to have valid outliers, I've gone with a heuristic approach, choosing to look at the distance between the uppermost outliers for each attribute and it's nearest neighbour.

```{r}
#Create a list to populate with our tail neighbour distances
tail_deltas <- list()

for (attrib in wine.scaled) {
  #get the last two values
  data_tails <- tail(sort(attrib),2)
  #push the delta on to our list 
  tail_deltas <- append(tail_deltas, diff(data_tails))
}

#We seem to be creating a vector of vectors to we need to flatten this (TODO: Fix if we have time!)
flattened_delta_list <- unlist(tail_deltas, recursive=FALSE)

#grab out attribute keys to include in our new table/frame
attributes <- names(wine.scaled)

#make a new dataframe from 
dataframe <- data.frame(attributes = attributes, tail_neighbour_d=flattened_delta_list)

#get the order for the nearest neighbour starting with the greatest distance and descending
neighbout_order <- order(dataframe$tail_neighbour_d, decreasing=TRUE)

#now apply the order to the frame
dataframe[ neighbout_order, ]

```

Given the findings, I think we can just consider the top five attributes in the above list as ones to cleanse for outliers. A lot of sources online warn against arbitrarily getting rid of outliers because it might be the case that valid information is being lost when what you really wnat to be account for is bad data.

To clarify, the attrbibutes to be processed are:
- density			
-	free sulfur dioxide			
-	residual sugar		
-	citric acid 		
-	fixed acidity

```{r}
wine.scaled_cleansed <- wine.scaled[ !(wine.scaled$density %in% boxplot(wine.scaled$density, plot=FALSE)$out), ]
wine.scaled_cleansed <- wine.scaled_cleansed[ !(wine.scaled_cleansed$`free sulfur dioxide` %in% boxplot(wine.scaled$`free sulfur dioxide`, plot=FALSE)$out), ]
wine.scaled_cleansed <- wine.scaled_cleansed[ !(wine.scaled_cleansed$`residual sugar` %in% boxplot(wine.scaled_cleansed$`residual sugar`, plot=FALSE)$out), ]
wine.scaled_cleansed <- wine.scaled_cleansed[ !(wine.scaled_cleansed$`citric acid` %in% boxplot(wine.scaled_cleansed$`citric acid`, plot=FALSE)$out), ]
wine.scaled_cleansed <- wine.scaled_cleansed[ !(wine.scaled_cleansed$`fixed acidity` %in% boxplot(wine.scaled_cleansed$`fixed acidity`, plot=FALSE)$out), ]

boxplot(wine.scaled_cleansed, main="Looking at the cleansed data graphically", xlab="Wine Attributes", ylab="Scalled values") 

```

Now that the data looks a lot cleaner, it's time to start working with the data to try and find the best clustering. To begin with, nbclust will be used to see if that produces anything useful.

```{r}
set.seed(123)

number_of_clusters <- NbClust(wine.scaled_cleansed,
                 min.nc=2, max.nc=15,
                 method="kmeans")

table(number_of_clusters$Best.n[1,])
```

```{r}
barplot(table(number_of_clusters$Best.n[1,]), 
        xlab="Numer of Clusters",
        ylab="Number of Criteria",
        main="Number of Clusters Chosen by 30 Criteria")
```

To confirm that the accuracy of this result, we can plot the sum of square errors and looks for a ponounced bend in the graph. Where the most pronounced bend is, this is a contender for the value for k.

```{r}
sse_list <- 0
for (i in 1:15){
  sse_list[i] <- sum(kmeans(wine.scaled_cleansed, centers=i)$withinss)
}

plot(1:15,
  sse_list,
  type="b",
  xlab="Number of Clusters",
  ylab="Within groups sum of squares")
```

The histogram backs up the results of nbclust seeing as the 'elbow' of the line as at 2 on the Number of Clusters. So 

```{r}
fit.km <- kmeans(data.train, 2)

```

## R Markdown


## Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
