---
title: "Forecasting1"
output: html_document
---


```{r setup-f1, include=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r deps-f1, echo=FALSE, include=FALSE, message=FALSE}
mirrorUrl = "http://cran.ma.imperial.ac.uk"

# Install and load all packages up-front!
if(!require(readxl)) install.packages("readxl", repos = mirrorUrl)
if(!require(neuralnet)) install.packages("neuralnet", repos = mirrorUrl)
if(!require(Metrics)) install.packages("Metrics", repos = mirrorUrl)
if(!require(scales)) install.packages("scales", repos = mirrorUrl)
if(!require(data.table)) install.packages("data.table", repos = mirrorUrl)
#
library("readxl")
library("neuralnet")
library("Metrics")
library("scales")
library("data.table")

set.seed(1234)
```

<section>

#Question 3: Forecasting (MLP)

##Premise

>You need to construct an MLP neural network for this problem. You need to consider the appropriate input vector, as well as the internal network structure (hidden layers, nodes, learning rate). You may consider any de-trending scheme if you feel is necessary. Write a code in R Studio to address all these requirements. You need to show the performance of your network both graphically as well as in terms of usual statistical indices (MSE, RMSE and MAPE). Hint: Experiment with various network structures and show a comparison table of their performances. This will be a good justification for your final network choice. Show all your working steps. As everyone will have different forecasting result, emphasis in the marking scheme will be given to the adopted methodology and the explanation/justification of various decisions you have taken in order to provide an acceptable, in terms of performance, solution. The input selection problem is very important. Experiment with various options (i.e. how many past values you need to consider as potential network inputs).

<!--
3rd Objective (MLP)
• Discuss the input selection problem and propose various input configurations 10
• Design a number of MLPs, using various structures (layers/nodes) / input parameters and show in a table their performances comparison based on provided stat. indices 15
• Provide your best results both graphically (your prediction output vs. desired output) and via performance indices 5

resources:
-->

##Preparation of data

The exchange data needs to be loaded, partitioned (into training and testing datasets), and scaled [@RefWorks:doc:5a0f45afe4b0373bf38bf6a2].

```{r prep-data-f1}
#going to import the Excel spreadsheet Currency Exchange dataset
exchange.raw <- read_excel("../data/Exchange.xlsx")
```

Here's a glance at the dataset

```{r show-exchange-f1}
head(exchange.raw)
str(exchange.raw)
```

We want to scale the data to allow for faster training. 

```{r norm-func}
norm_func <- function(x){
     min_x <- min(x)

     return((x - min_x)/(max(x) - min_x))
} 
```
```{r scale-exchange-f1}
exchange.scaled <- exchange.raw

exchange.scaled[3] <- scale(exchange.scaled[3])

#Summary of scaled wine data
summary(exchange.scaled)
```
```{r normalise-exchange-f1}
exchange.normalised <- exchange.raw

exchange.normalised[3] <- norm_func(exchange.normalised[3])

#Summary of scaled wine data
summary(exchange.normalised)
```

And partition the data into a set for training and another for testing the neural network after.

```{r subset-exchange-f1}
exchange.scaled_train <- head(exchange.scaled, 320)
exchange.scaled_test <- tail(exchange.scaled, -320) #exchange.scaled[-1:-320, 1:3] #also works!


#Summary of scaled data
summary(exchange.scaled_train)
summary(exchange.scaled_test)
```
```{r subset-norm-exchange-f1}
exchange.normalised_train <- head(exchange.normalised, 320)
exchange.normalised_test <- tail(exchange.normalised, -320) #exchange.normalised[-1:-320, 1:3] #also works!


#Summary of normalised data
paste("training:")
summary(exchange.normalised_train)
paste("test:")
summary(exchange.normalised_test)
```

### The time series input problem

Because we're forecasting with time-series data, one of the main concerns when choosing the configuration of the neural network is how many input values are fed into the system in order to derive our output; too few inputs and we lack the data for forecasting but too many and the neural network. In the following process, the number of inputs will be on variable that is experimented with along side the number of hidden layers and back propagation techniques.

###Setting up the training data

Because we're looking to train the neural network on time-series data, we have to transform the data to make it easy to pass in time based input, which means we need to provide more than 1 input value at a time to train against the desired output; to that end, below is a code chunk that takes the exchange rate values and creates rows where the last entry is the desired output, the third is considered day 0 and the 1st to entries are 2 previous days from day 0.

Before the data is ready, a function can be created in order to re-use the functionality over and over; this way the same code can be applied to generating out test data as the training data.


```{r matrix-func-f1}
vector_to_time_series_data <- function(vec, colCount, step = 1) {
  row_count <- length(vec)
  
  staggered_data_matrix <- matrix(vec, row_count, colCount)
  
  for (i in 1:row_count){
    new_row <- c(staggered_data_matrix[i])

    for (j in 1:(colCount-1)){
      new_row <- c(new_row, staggered_data_matrix[i + (step * j)])
    }
    
    staggered_data_matrix[i,] <- new_row
  }

  return (as.data.frame(head(staggered_data_matrix, row_count - ((colCount -1) * step))))
}
```

With this function we can transform the training currency values now:

```{r matrix-exchange-f1}
staggered_data_frame <- vector_to_time_series_data(exchange.scaled_train$`USD/EUR`, 4)

colnames(staggered_data_frame) <- c("Input_dneg2", "Input_dneg1", "Input_d0", "Output")

#Summary of training data
head(staggered_data_frame)
```

##Creating & using the neural net

###Training the network

Now we have the data in a format such that we can provide 3 consecutive days as input and the following day as desired output, we are ready to train the neural network.

```{r train-mlp-f1}
mlp.form1 <- as.formula("Output ~ Input_d0 + Input_dneg1 + Input_dneg2")

mlp.nn1 <- neuralnet(mlp.form1, staggered_data_frame, hidden=c(8,4,2), threshold=0.01)
```

Now the neural network has been trained we can view a representation of its structure.

```{r display-mlp-f1}
plot(mlp.nn1)
```

###Testing the network

Before we can test the neural network against test data we need to transform the test data we took from the original dataset and transform it so we can pass in three input variables similarly to how the network was trained. Picking the number of inputs and what those inputs might be is part of the challenge of creating a neural network for time-series data; it's pretty obvious that one input value will hardly help project a future value but what value to pick isn't clear. The lower the number or inputs the greater likelihood of error but the higher the number of inputs, the increased complexity and longer training period. I've picked 3 consecutive days to start with but will experiment with other configurations afterwards.

```{r prep-test-mlp-f1}
staggered_test_data_frame <- vector_to_time_series_data(exchange.scaled_test$`USD/EUR`, 3)
```

Now that the test data is arranged in the same way that the training data is, the neural network can be tested.

```{r make_comparison_table_func-f1}
expected_v_test_func <- function (expected, mlp_test) {
  expected_v_test <- cbind(expected, as.data.frame(head(mlp_test$net.result, -1)))
  colnames(expected_v_test) <- c("Expected Output", "Neural Net Output")
  return(expected_v_test)
}
```
```{r testing-mlp-f1}
mlp.nn1_results <- compute(mlp.nn1, staggered_test_data_frame)

test_expected_data.nn1 <- tail(exchange.scaled_test$`USD/EUR`, -3)

test_v_expected.nn1 <- expected_v_test_func(test_expected_data.nn1, mlp.nn1_results)
head(test_v_expected.nn1)
```

### Evaluating the predictions

There are two ways we can look at the quality of the performance of the neural network:
- Various single number values derived from the difference between the predicted and actual values
- Visualising the data, for example, by plotting the estimates and actual values on the same graph to more easily draw comparisons by eye.

#### Numeric indicators

And then the Sum of Square Errors, and Mean Squared Error values can be derived in order to look at the performance of the trained neural network against test data. The closer to 0 the values are, the better [@RefWorks:doc:5a1058c5e4b0eb1ce060a9f3].

```{r indicator-funcs1-mlp-f1}
output_delta_func <- function (expected, actual) {
  return (c(expected-actual)) 
}
sse_func <- function(x) {
   return (sum( (x - mean(x) )^2 ))
}
mse_func <- function (x) {
  return(sse_func(x)/length(x))
}
```
```{r run-indicator-funcs1-mlp-f1}
test_delta.nn1 <- output_delta_func(test_v_expected.nn1$`Expected Output`, test_v_expected.nn1$`Neural Net Output`)
#SSE of this first nn
t_sse <- sse_func(test_delta.nn1)
t_sse
#MSE of this first nn
t_mse <- mse_func(test_delta.nn1)
t_mse

performance_scores <- list(nn1 = list(t_sse, t_mse))
```

Additionally we can look are the performance in terms of the Root Mean Square Error or the Mean Absolute Percentage Error. The Root Mean Square Error is the square root of the average of the squared errors, which allows for the greater individual errors to have a greater influence on the final error value. The Mean Absolute Percentage Error shows the error in terms of the the median of the errors as a fraction of the estimated value, which is displayed as a percentage; the biggest drawback to this measure is the risk of dividing by 0 is the estimated value is 0 but is doesn't allow the biggest deltas to influence the final value more than by their proportions.

What follows are the RMSE and MAPE values for the neural net.

```{r run-indicator2-mlp-f1}
#RMSE for the nn
t_rmse <- rmse(test_v_expected.nn1$`Neural Net Output`, test_v_expected.nn1$`Expected Output`)
t_rmse
#MAPE for the nn
t_mape <- percent(mape(test_v_expected.nn1$`Neural Net Output`, test_v_expected.nn1$`Expected Output`))
t_mape

performance_scores$nn1 <- append(performance_scores$nn1, c(t_rmse, t_mape))
```

#### Visual evaluation

That follows is a graph plotting neural net predicted values against actual (normalised/scaled) values.


```{r line-graph-func-mlp-f1}
line_compare_nn_to_actual <- function (compare_data, token_x, suffix = "") {
  count.col <- ncol(compare_data)

  # get the range for the x and y axis 
  plot_range.y <- range(compare_data)
  plot_range.x <- range(1:nrow(compare_data))
  
  #print(c("plot_range.y:",  plot_range.y, " - "))
  #print(c("compare_data:",  compare_data, " - "))
  
  # set up the plot 
  plot(token_x, type="n", xlab="Days", ylab="Normalised Exchange Rate") 
  
  colors <- c(6, 4)
  linetype <- c(1:count.col) 
  plotchar <- seq(1, 0)
  
  # add lines 
  for (i in 1:count.col) { 
    lines(compare_data[i], type="b", lwd=1.5,
      lty=linetype[i], col=colors[i], pch=plotchar[i]) 
  } 
  
  # add a title and subtitle 
  title(paste(c("Exchange Rate Prediction Versus Actual", suffix)),
        "Comparing actual values for USD/EUR rates against values derived from a Neural Network")
  
  return(list(linetype, plotchar, colors))
}
```
```{r line-graph-mlp-f1}
# Create Line Chart
linetypes <- line_compare_nn_to_actual(test_v_expected.nn1, test_v_expected.nn1$`Expected Output`, suffix = "(NN1)")

# add a legend 
legend(45, -.25, names(test_v_expected.nn1), cex=0.8, col=linetypes[[3]], pch=linetypes[[2]], lty=linetypes[[1]], title="Exchange Rates")
```

What's interesting about looking at it visually, is how it's more obvious from this perspective than it is from indicators values, that the neural net predictions are similar overall but with a latency of a few days; this isn't very useful for forecasting and might actually suggest a kind of over-fitting, where the network has been too tightly trained with the training data. I'm not certain that this is the case however because this graph is based on data that was not used for training purpose.

## Experimenting with other Neural Network configurations

Now that we have one neural net with performance results, it's time to use this as a benchmark/baseline and create other neural networks to see if we can improve on performance.

### Normalised data instead of scaled data

I'm just curious as to whether or not normalising the data between 0 and 1 would work better than between -1 and 1 around a mean of 0.

```{r matrix-exchange-normalised-f1}
normalised_staggered_data_frame <- vector_to_time_series_data(exchange.normalised_train$`USD/EUR`, 4)

colnames(normalised_staggered_data_frame) <- c("Input_dneg2", "Input_dneg1", "Input_d0", "Output")

#Summary of training data
head(normalised_staggered_data_frame)
```
```{r train-mlp-norm-f1}
mlp.form1 <- as.formula("Output ~ Input_d0 + Input_dneg1 + Input_dneg2")
mlp.nn1 <- neuralnet(mlp.form1, staggered_data_frame, hidden = c(8,4,2), threshold=0.01)

mlp.nn1_norm <- neuralnet(mlp.form1, normalised_staggered_data_frame, hidden=c(8,4,2), threshold=0.01)
```

```{r display-mlp-norm-f1}
plot(mlp.nn1_norm)
```

```{r prep-test-mlp-norm-f1}
normalised_staggered_test_data_frame <- vector_to_time_series_data(exchange.normalised_test$`USD/EUR`, 3)

colnames(normalised_staggered_test_data_frame) <- c("Input_dneg2", "Input_dneg1", "Input_d0")

#Summary of training data
head(normalised_staggered_test_data_frame)
```

```{r testing-mlp-norm-f1}
mlp.nn1_norm_results <- compute(mlp.nn1_norm, normalised_staggered_test_data_frame)

test_expected_data.nn1_norm <- tail(exchange.normalised_test$`USD/EUR`, -3)

test_v_expected.nn1_norm <- expected_v_test_func(test_expected_data.nn1_norm, mlp.nn1_norm_results)
head(test_v_expected.nn1_norm)
```
```{r run-indicator-funcs1-mlp-norm-f1}
test_delta.nn1_norm <- output_delta_func(test_v_expected.nn1_norm$`Expected Output`, test_v_expected.nn1_norm$`Neural Net Output`)
#SSE of this first nn
t_sse <- sse_func(test_delta.nn1_norm)
t_sse
#MSE of this first nn
t_mse <- mse_func(test_delta.nn1_norm)
t_mse
#RMSE for the nn
t_rmse <- rmse(test_v_expected.nn1_norm$`Neural Net Output`, test_v_expected.nn1_norm$`Expected Output`)
t_rmse
#MAPE for the nn
t_mape <- percent(mape(test_v_expected.nn1_norm$`Neural Net Output`, test_v_expected.nn1_norm$`Expected Output`))
t_mape

performance_scores <- append(performance_scores, list(nn1_norm = list(t_sse, t_mse, t_rmse, t_mape)))

```

I'm not really sure what's going on with the line graph being plot for this but the figures show that the use of normalised data over scaled data is a marked improvement; the number of steps to train the network is faster, the accuracy is higher against test data; as such, I will make sure all other neural networks for this experiment will use the normalised data.

### Alternative hidden layer structures

The hidden layers are the 'secret sauce' of this kind of neural network, the layers describe neurons consisting of nodes and vertices that connect to other neuron nodes; the vertices between nodes adjust the values as they are passed from node to node resulting in the answer at the end of the chain off layers. The art is to pick a good combination of hidden layers that doesn't over-fit to the training data and isn't so complicated it take a long time to train but also produces good accuracy results against test and validation data [@RefWorks:doc:5a14aa82e4b08b76da9720f5]. The first neural network had 3 hidden layers each powers of 2, starting with 8 and halving at each next layer.

The next step is to experiment with the same dataset and the same input values but experiment with the accuracy of some different hinder layer structures like:

1. 6, 9, 6
2. 6, 9, 6, 3
3. 12, 8, 4
4. 16, 9, 4

### {6, 9, 6} hidden layer structure

```{r train-mlp-6-9-6-f1}
#mlp.form1 <- as.formula("Output ~ Input_d0 + Input_dneg1 + Input_dneg2")

mlp.nn_6_9_6 <- neuralnet(mlp.form1, normalised_staggered_data_frame, hidden = c(6, 9, 6), threshold=0.05, stepmax = 800000)
```
```{r display-mlp-6-9-6-f1}
plot(mlp.nn_6_9_6)
```

```{r comparison-table-mlp-6-9-6-f1}
mlp.nn2_results <- compute(mlp.nn_6_9_6, normalised_staggered_test_data_frame)

test_expected_data.nn2 <- tail(exchange.normalised_test$`USD/EUR`, -3)

test_v_expected.nn2 <- expected_v_test_func(test_expected_data.nn2, mlp.nn2_results)
```
```{r run-indicator-funcs-mlp-6-9-6-f1}
test_delta.nn2 <- output_delta_func(test_v_expected.nn2$`Expected Output`, test_v_expected.nn2$`Neural Net Output`)
#SSE of this first nn
t_sse <- sse_func(test_delta.nn2)
t_sse
#MSE of this first nn
t_mse <- mse_func(test_delta.nn2)
t_mse
#RMSE for the nn
t_rmse <- rmse(test_v_expected.nn2$`Neural Net Output`, test_v_expected.nn2$`Expected Output`)
t_rmse
#MAPE for the nn
t_mape <- percent(mape(test_v_expected.nn2$`Neural Net Output`, test_v_expected.nn2$`Expected Output`))
t_mape

performance_scores <- append(performance_scores, list(nn2 = list(t_sse, t_mse, t_rmse, t_mape)))

```

### {6, 9, 6, 3} hidden layer structure

```{r train-mlp-6-9-6-3-f1}
#mlp.form1 <- as.formula("Output ~ Input_d0 + Input_dneg1 + Input_dneg2")

mlp.nn_6_9_6_3 <- neuralnet(mlp.form1, normalised_staggered_data_frame, hidden = c(6, 9, 6, 3), threshold=0.05, stepmax = 1200000)
```
```{r display-mlp-6-9-6-3-f1}
plot(mlp.nn_6_9_6_3)
```

```{r comparison-table-mlp-6-9-6-3-f1}
mlp.nn3_results <- compute(mlp.nn_6_9_6_3, normalised_staggered_test_data_frame)

test_expected_data.nn3 <- tail(exchange.normalised_test$`USD/EUR`, -3)

test_v_expected.nn3 <- expected_v_test_func(test_expected_data.nn3, mlp.nn3_results)
```
```{r run-indicator-funcs-mlp-6-9-6-3-f1}
test_delta.nn3 <- output_delta_func(test_v_expected.nn3$`Expected Output`, test_v_expected.nn3$`Neural Net Output`)
#SSE of this first nn
t_sse <- sse_func(test_delta.nn3)
t_sse
#MSE of this first nn
t_mse <- mse_func(test_delta.nn3)
t_mse
#RMSE for the nn
t_rmse <- rmse(test_v_expected.nn3$`Neural Net Output`, test_v_expected.nn3$`Expected Output`)
t_rmse
#MAPE for the nn
t_mape <- percent(mape(test_v_expected.nn3$`Neural Net Output`, test_v_expected.nn3$`Expected Output`))
t_mape

performance_scores <- append(performance_scores, list(nn3 = list(t_sse, t_mse, t_rmse, t_mape)))

```

### {12, 8, 4} hidden layer structure

```{r train-mlp-12-8-4-f1}
#mlp.form1 <- as.formula("Output ~ Input_d0 + Input_dneg1 + Input_dneg2")

mlp.nn_12_8_4 <- neuralnet(mlp.form1, normalised_staggered_data_frame, hidden = c(12, 8, 4), threshold=0.05, stepmax = 1200000)
```
```{r display-mlp-12-8-4-f1}
plot(mlp.nn_12_8_4)
```

```{r comparison-table-mlp-mlp.nn-12-8-4-f1}
mlp.nn4_results <- compute(mlp.nn_12_8_4, normalised_staggered_test_data_frame)

test_expected_data.nn4 <- tail(exchange.normalised_test$`USD/EUR`, -3)

test_v_expected.nn4 <- expected_v_test_func(test_expected_data.nn4, mlp.nn4_results)
```
```{r run-indicator-funcs-mlp-12-8-4-f1}
test_delta.nn4 <- output_delta_func(test_v_expected.nn4$`Expected Output`, test_v_expected.nn4$`Neural Net Output`)
#SSE of this first nn
t_sse <- sse_func(test_delta.nn4)
t_sse
#MSE of this first nn
t_mse <- mse_func(test_delta.nn4)
t_mse
#RMSE for the nn
t_rmse <- rmse(test_v_expected.nn4$`Neural Net Output`, test_v_expected.nn4$`Expected Output`)
t_rmse
#MAPE for the nn
t_mape <- percent(mape(test_v_expected.nn4$`Neural Net Output`, test_v_expected.nn4$`Expected Output`))
t_mape

performance_scores <- append(performance_scores, list(nn4 = list(t_sse, t_mse, t_rmse, t_mape)))

```

### {16, 9, 4} hidden layer structure

```{r train-mlp-16-9-4-f1}
#mlp.form1 <- as.formula("Output ~ Input_d0 + Input_dneg1 + Input_dneg2")

mlp.nn_16_9_4 <- neuralnet(mlp.form1, normalised_staggered_data_frame, hidden = c(16, 9, 4), threshold=0.05, stepmax = 1200000)
```

```{r display-mlp-16-9-4-f1}
plot(mlp.nn_16_9_4)
```

```{r comparison-table-mlp-mlp.nn-16-9-4-f1}
mlp.nn5_results <- compute(mlp.nn_16_9_4, normalised_staggered_test_data_frame)

test_expected_data.nn5 <- tail(exchange.normalised_test$`USD/EUR`, -3)

test_v_expected.nn5 <- expected_v_test_func(test_expected_data.nn5, mlp.nn5_results)
```
```{r run-indicator-funcs-mlp-16-9-4-f1}
test_delta.nn5 <- output_delta_func(test_v_expected.nn5$`Expected Output`, test_v_expected.nn5$`Neural Net Output`)
#SSE of this first nn
t_sse <- sse_func(test_delta.nn5)
t_sse
#MSE of this first nn
t_mse <- mse_func(test_delta.nn5)
t_mse
#RMSE for the nn
t_rmse <- rmse(test_v_expected.nn5$`Neural Net Output`, test_v_expected.nn5$`Expected Output`)
t_rmse
#MAPE for the nn
t_mape <- percent(mape(test_v_expected.nn5$`Neural Net Output`, test_v_expected.nn5$`Expected Output`))
t_mape

performance_scores <- append(performance_scores, list(nn5 = list(t_sse, t_mse, t_rmse, t_mape)))
```

### Performance Thus Far
```{r performance-mlp-structures}
# Display a table for performance so far
perf_dt_1 <- rbindlist(performance_scores)
perf_dt_1 <- cbind(names(performance_scores), perf_dt_1)
colnames(perf_dt_1) <- c("NN", "SSE", "MSE", "RMSE", "MAPE")
perf_dt_1
```

Looking at the performance metrics so far, it would make sense to invest more time experimenting with neural networks with the following hidden layers: {6, 9, 6} (nn2), {12, 8, 4} (nn4); for these two structures, all the metrics outperform the other neural networks in terms of being closest to 0. What needs to be seen now is whether of not the accuracy of the networks can be improved by increasing or decreasing the number of inputs. It would make sense that accuracy should increase with greater number of inputs but for the sake of completeness, 2 inputs will be tried as well as 4.

##Alternative Input Counts

In order to experiment with the inputs we need to first prepare the data.

### Preparing the data for 2 inputs
```{r matrix-exchange-normalised-2i-f1}
normalised_staggered_2i_data_frame <- vector_to_time_series_data(exchange.normalised_train$`USD/EUR`, 3)

colnames(normalised_staggered_2i_data_frame) <- c("Input_dneg1", "Input_d0", "Output")

#Summary of training data
head(normalised_staggered_2i_data_frame)
```
```{r prep-test-mlp-norm-2i-f1}
normalised_staggered_2i_test_data_frame <- vector_to_time_series_data(exchange.normalised_test$`USD/EUR`, 2)

colnames(normalised_staggered_2i_test_data_frame) <- c("Input_dneg1", "Input_d0")

#Summary of training data
head(normalised_staggered_2i_test_data_frame)
```

### Preparing the data for 4 inputs
```{r matrix-exchange-normalised-4i-f1}
normalised_staggered_4i_data_frame <- vector_to_time_series_data(exchange.normalised_train$`USD/EUR`, 5)

colnames(normalised_staggered_4i_data_frame) <- c("Input_dneg3", "Input_dneg2", "Input_dneg1", "Input_d0", "Output")

#Summary of training data
head(normalised_staggered_4i_data_frame)
```
```{r prep-test-mlp-norm-4i-f1}
normalised_staggered_4i_test_data_frame <- vector_to_time_series_data(exchange.normalised_test$`USD/EUR`, 4)

colnames(normalised_staggered_4i_test_data_frame) <- c("Input_dneg3", "Input_dneg2", "Input_dneg1", "Input_d0")

#Summary of training data
head(normalised_staggered_4i_test_data_frame)
```

### {6, 9, 6} hidden layer structure with 2 inputs

```{r train-mlp-2i-6-9-6-f1}
mlp.form2 <- as.formula("Output ~ Input_d0 + Input_dneg1")

mlp.nn_6_9_6_2i <- neuralnet(mlp.form2, normalised_staggered_2i_data_frame, hidden = c(6, 9, 6), threshold=0.05, stepmax = 800000)
```
```{r display-mlp-2i-6-9-6-f1}
plot(mlp.nn_6_9_6_2i)
```

```{r comparison-table-mlp-2i-6-9-6-f1}
mlp.nn2_2i_results <- compute(mlp.nn_6_9_6_2i, normalised_staggered_2i_test_data_frame)

test_expected_data.nn2_2i <- tail(exchange.normalised_test$`USD/EUR`, -2)

test_v_expected.nn2_2i <- expected_v_test_func(test_expected_data.nn2_2i, mlp.nn2_2i_results)
```
```{r run-indicator-funcs-mlp-2i-6-9-6-f1}
test_delta.nn2_2i <- output_delta_func(test_v_expected.nn2_2i$`Expected Output`, test_v_expected.nn2_2i$`Neural Net Output`)
#SSE of this first nn
t_sse <- sse_func(test_delta.nn2_2i)
t_sse
#MSE of this first nn
t_mse <- mse_func(test_delta.nn2_2i)
t_mse
#RMSE for the nn
t_rmse <- rmse(test_v_expected.nn2_2i$`Neural Net Output`, test_v_expected.nn2_2i$`Expected Output`)
t_rmse
#MAPE for the nn
t_mape <- percent(mape(test_v_expected.nn2_2i$`Neural Net Output`, test_v_expected.nn2_2i$`Expected Output`))
t_mape

performance_scores <- append(performance_scores, list(nn2_2i = list(t_sse, t_mse, t_rmse, t_mape)))

```

### {6, 9, 6} hidden layer structure with 4 inputs

```{r train-mlp-4i-6-9-6-f1}
mlp.form4 <- as.formula("Output ~ Input_d0 + Input_dneg1 + Input_dneg2 + Input_dneg3")

mlp.nn_6_9_6_4i <- neuralnet(mlp.form4, normalised_staggered_4i_data_frame, hidden = c(6, 9, 6), threshold=0.05, stepmax = 800000)
```
```{r display-mlp-4i-6-9-6-f1}
plot(mlp.nn_6_9_6_4i)
```

```{r comparison-table-mlp-4i-6-9-6-f1}
mlp.nn2_4i_results <- compute(mlp.nn_6_9_6_4i, normalised_staggered_4i_test_data_frame)

test_expected_data.nn2_4i <- tail(exchange.normalised_test$`USD/EUR`, -4)

test_v_expected.nn2_4i <- expected_v_test_func(test_expected_data.nn2_4i, mlp.nn2_4i_results)
```
```{r run-indicator-funcs-mlp-4i-6-9-6-f1}
test_delta.nn2_4i <- output_delta_func(test_v_expected.nn2_4i$`Expected Output`, test_v_expected.nn2_4i$`Neural Net Output`)
#SSE of this first nn
t_sse <- sse_func(test_delta.nn2_4i)
t_sse
#MSE of this first nn
t_mse <- mse_func(test_delta.nn2_4i)
t_mse
#RMSE for the nn
t_rmse <- rmse(test_v_expected.nn2_4i$`Neural Net Output`, test_v_expected.nn2_4i$`Expected Output`)
t_rmse
#MAPE for the nn
t_mape <- percent(mape(test_v_expected.nn2_4i$`Neural Net Output`, test_v_expected.nn2_4i$`Expected Output`))
t_mape

performance_scores <- append(performance_scores, list(nn2_4i = list(t_sse, t_mse, t_rmse, t_mape)))

```

### Performance After Input Experimentation
```{r performance-mlp-structures-update}
# Display a table for performance so far
perf_dt_2 <- rbindlist(performance_scores)
perf_dt_2 <- cbind(names(performance_scores), perf_dt_2)
colnames(perf_dt_2) <- c("NN", "SSE", "MSE", "RMSE", "MAPE")
perf_dt_2
```

### A closer look at the better performers

The following is a graph looking at the {12, 8, 4} hidden layer neural network, also labelled 'nn4'; this was the second best outcome of the neural networks that used 3 inputs.

```{r line-graph-mlp-12-8-4-f1}
# Create Line Chart
linetypes <- line_compare_nn_to_actual(test_v_expected.nn4, test_v_expected.nn4$`Expected Output`, suffix = "(NN4)")

# add a legend 
legend(45, -.25, names(test_v_expected.nn4), cex=0.8, col=linetypes[[3]], pch=linetypes[[2]], lty=linetypes[[1]], title="Exchange Rates")
```

The following is a graph looking at the {6, 9, 6} hidden layer neural network where the number of inputs was reduced to 2, also labelled 'nn2_2i'; this was the second best outcome of the neural networks that used 3 inputs.

```{r line-graph-mlp-2i-6-9-6-f1}
# Create Line Chart
linetypes <- line_compare_nn_to_actual(test_v_expected.nn2_2i, test_v_expected.nn2_2i$`Expected Output`, suffix = "(NN2 - 2 inputs)")

# add a legend 
legend(0, .75, names(test_v_expected.nn2_2i), cex=0.8, col=linetypes[[3]], pch=linetypes[[2]], lty=linetypes[[1]], title="Exchange Rates")
```

The graphs tell a truth that the indicator values do not; there seems to be some serious over-fitting going on. The neural network results look like that they're mostly predicting based on the value of the previous day which is not really going to be much use in terms of real forecasting. There are are a few more courses of action we cane take to see if we can get better results:

- Look into tweaking other variables for the Neural Network behaviour
  - Specifically the learning algorithm and learning rate, like using ’rprop-’ instead of ’rprop+’
- Look into feeding in different data
  - Like increasing the interval of days between input values 
- Using a different Neural Network library to see if it's more suitable for time-series data

## More experimentation

### Adjusting threshold and algorithm

The next experiment is to drastically increase the threshold tolerance and choose a variation of the back propagation algorithm, 'rprop-'.

```{r train-mlp-12-8-4-t0p5-f1}
#mlp.form1 <- as.formula("Output ~ Input_d0 + Input_dneg1 + Input_dneg2")

mlp.nn_12_8_4_t0p5 <- neuralnet(mlp.form1, normalised_staggered_data_frame, hidden = c(12, 8, 4), threshold=0.5, algorithm = "rprop-")
```

```{r comparison-table-mlp-mlp.nn-12-8-4-t0p5-f1}
mlp.nn4_t0p1_results <- compute(mlp.nn_12_8_4_t0p5, normalised_staggered_test_data_frame)

test_expected_data.nn4_t0p1 <- tail(exchange.normalised_test$`USD/EUR`, -3)

test_v_expected.nn4_t0p5 <- expected_v_test_func(test_expected_data.nn4_t0p1, mlp.nn4_t0p1_results)
```

```{r run-indicator-funcs-mlp-12-8-4-t0p5-f1}
test_delta.nn4_t0p5 <- output_delta_func(test_v_expected.nn4_t0p5$`Expected Output`, test_v_expected.nn4_t0p5$`Neural Net Output`)
#SSE of this first nn
t_sse <- sse_func(test_delta.nn4_t0p5)
t_sse
#MSE of this first nn
t_mse <- mse_func(test_delta.nn4_t0p5)
t_mse
#RMSE for the nn
t_rmse <- rmse(test_v_expected.nn4_t0p5$`Neural Net Output`, test_v_expected.nn4_t0p5$`Expected Output`)
t_rmse
#MAPE for the nn
t_mape <- percent(mape(test_v_expected.nn4_t0p5$`Neural Net Output`, test_v_expected.nn4_t0p5$`Expected Output`))
t_mape

#performance_scores <- append(performance_scores, list(nn4 = list(t_sse, t_mse, t_rmse, t_mape)))

```

```{r line-graph-mlp-12-8-4-t0p1-f1}
# Create Line Chart
linetypes <- line_compare_nn_to_actual(test_v_expected.nn4_t0p5, test_v_expected.nn4_t0p5$`Expected Output`, suffix = "(NN4 - 0.5 threshold)")

# add a legend 
legend(0, .75, names(test_v_expected.nn4_t0p5), cex=0.8, col=linetypes[[3]], pch=linetypes[[2]], lty=linetypes[[1]], title="Exchange Rates")
```

Lessening the threshold for training actually allows the neural network to not fixate so much on the value but instead the trends to this graph demonstrates better performance but it's still not achieving the sort of results we'd hope for.

### Adjusting input training data

The next experiment is to see if giving the Neural Network broader data it might better predict the expected value; in this case training with days that are a day apart.

Here follows the training data:
```{r matrix-exchange-2day-f1}
normalised_staggered_2day_data_frame <- vector_to_time_series_data(exchange.normalised_train$`USD/EUR`, 4, step=2)

colnames(normalised_staggered_2day_data_frame) <- c("Input_dneg6", "Input_dneg4", "Input_dneg2", "Output")

#Summary of training data
head(normalised_staggered_2day_data_frame)
```

Here follows the test data:
```{r prep-test-mlp-norm-2day-f1}
normalised_staggered_test_2day_data_frame <- vector_to_time_series_data(exchange.normalised_test$`USD/EUR`, 3, step = 2)

colnames(normalised_staggered_test_2day_data_frame) <- c("Input_dneg6", "Input_dneg4", "Input_dneg2")

#Summary of training data
head(normalised_staggered_test_2day_data_frame)
```

```{r train-mlp-12-8-4-3id2-f1}
mlp.form_2d <- as.formula("Output ~ Input_dneg2 + Input_dneg4 + Input_dneg6")

mlp.nn_12_8_4_3id2 <- neuralnet(mlp.form_2d, normalised_staggered_2day_data_frame, hidden = c(12, 8, 4), threshold=0.5, algorithm = "rprop-")
```

```{r comparison-table-mlp-mlp.nn-12-8-4-3id2-f1}
mlp.nn4_3id2_results <- compute(mlp.nn_12_8_4_3id2, normalised_staggered_test_2day_data_frame)

test_expected_data.nn4_3id2 <- tail(exchange.normalised_test$`USD/EUR`, -5)

test_v_expected.nn4_3id2 <- expected_v_test_func(test_expected_data.nn4_3id2, mlp.nn4_3id2_results)
```

```{r run-indicator-funcs-mlp-12-8-4-3id2-f1}
test_delta.nn4_3id2 <- output_delta_func(test_v_expected.nn4_3id2$`Expected Output`, test_v_expected.nn4_3id2$`Neural Net Output`)
#SSE of this first nn
t_sse <- sse_func(test_delta.nn4_3id2)
t_sse
#MSE of this first nn
t_mse <- mse_func(test_delta.nn4_3id2)
t_mse
#RMSE for the nn
t_rmse <- rmse(test_v_expected.nn4_3id2$`Neural Net Output`, test_v_expected.nn4_3id2$`Expected Output`)
t_rmse
#MAPE for the nn
t_mape <- percent(mape(test_v_expected.nn4_3id2$`Neural Net Output`, test_v_expected.nn4_3id2$`Expected Output`))
t_mape

#performance_scores <- append(performance_scores, list(nn4 = list(t_sse, t_mse, t_rmse, t_mape)))

```

```{r line-graph-mlp-12-8-4-t0p1-2day-f1}
# Create Line Chart
linetypes <- line_compare_nn_to_actual(test_v_expected.nn4_3id2, test_v_expected.nn4_3id2$`Expected Output`, suffix = "(NN4 - 2 day gapped data)")

# add a legend 
legend(0, .75, names(test_v_expected.nn4_3id2), cex=0.8, col=linetypes[[3]], pch=linetypes[[2]], lty=linetypes[[1]], title="Exchange Rates")
```

Adjusting the input data to be over a spread of 6 days rather than 3 has not create the results I had been hoping for; my idea was that the neural network my find a more general trend of a longer time period but instead it seems to just lag behind even further; it's as though whatever is the most recent input value passed into the neural network is what then is the main determinant of the predicted value, without any kind of trajectory of extrapolation.

## Conclusion

I think that further research into other Neural Network libraries would be the first course of action I would take next, for example Caret and Nnet. The Neural network with the performance that pleased me the most was labelled `nn4_t0p5`, having a {12, 8, 4} hidden layer structure and a 0.5 accuracy threshold; this happens to have delivered best results (MAPE: 7.72%) when you take into account it wasn't so tightly over-fit to the data; I'd like to see how it would perform with more test data but I think another type of neural network might perform better.

One thing I didn't do, was to use any smoothing methods on the data to take into account noise and seasonality; if I had time, I would research how to do this and apply it to the training data to see if that provided any benefit, as I have a feeling it may well have! Another tool I would look into would be the auto-regressive moving average, which looks like another way of looking at recent trends as the time series moves along; perhaps even adding extra inputs that are my own derived values based on input data might help nudge the performance in the right direction; I won't know without trying.

</section>
