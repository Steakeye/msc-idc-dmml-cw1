---
title: "Forecasting1"
output: html_document
---


```{r setup-f1, include=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r deps-f1, echo=FALSE, include=FALSE, message=FALSE}
mirrorUrl = "http://cran.ma.imperial.ac.uk"

# Install and load all packages up-front!
if(!require(readxl)) install.packages("readxl", repos = mirrorUrl)
if(!require(neuralnet)) install.packages("neuralnet", repos = mirrorUrl)
if(!require(Metrics)) install.packages("Metrics", repos = mirrorUrl)
if(!require(scales)) install.packages("scales", repos = mirrorUrl)
#
library("readxl")
library("neuralnet")
library("Metrics")
library("scales")

set.seed(1234)
```

<section>

#Question 3: Forecasting (MLP)

##Premise

>You need to construct an MLP neural network for this problem. You need to consider the appropriate input vector, as well as the internal network structure (hidden layers, nodes, learning rate). You may consider any de-trending scheme if you feel is necessary. Write a code in R Studio to address all these requirements. You need to show the performance of your network both graphically as well as in terms of usual statistical indices (MSE, RMSE and MAPE). Hint: Experiment with various network structures and show a comparison table of their performances. This will be a good justification for your final network choice. Show all your working steps. As everyone will have different forecasting result, emphasis in the marking scheme will be given to the adopted methodology and the explanation/justification of various decisions you have taken in order to provide an acceptable, in terms of performance, solution. The input selection problem is very important. Experiment with various options (i.e. how many past values you need to consider as potential network inputs).

<!--
3rd Objective (MLP)
• Discuss the input selection problem and propose various input configurations 10
• Design a number of MLPs, using various structures (layers/nodes) / input parameters and show in a table their performances comparison based on provided stat. indices 15
• Provide your best results both graphically (your prediction output vs. desired output) and via performance indices 5

resources:
-->

##Preparation of data

The exchange data needs to be loaded, paritioned, and scaled.

```{r prep-data-f1}
#going to import the Excel spreadsheet WhiteWine dataset
exchange.raw <- read_excel("../data/Exchange.xlsx")
```

Here's a glance at the dataset

```{r show-exchange-f1}
head(exchange.raw)
str(exchange.raw)
```

We want to scale the data to allow  for faster training. 

```{r scale-exchange-f1}
exchange.scaled <- exchange.raw

exchange.scaled[3] <- scale(exchange.scaled[3])

#Summary of scaled wine data
summary(exchange.scaled)
```

And partition the data into a set for training and another for testing the neural network after.

```{r subset-exchange-f1}
exchange.scaled_train <- head(exchange.scaled, 320)
exchange.scaled_test <- tail(exchange.scaled, -320) #exchange.scaled[-1:-320, 1:3] #also works!


#Summary of scaled data
summary(exchange.scaled_train)
summary(exchange.scaled_test)
```

###Setting up the training data

Because we're looking to train the neural network on time-series data, we have to transform the data to make it easy to pass in time based input, which means we need to provide more than 1 input value at a time to train against the desired output; to that end, below is a code chunk that takes the exchange rate values and creates rows where the last entry is the desired output, the third is considered day 0 and the 1st to entries are 2 previous days from day 0.

Before the data is ready, a function can be created in order to re-use the functionality over and over; this way the same code can be applied to generating out test data as the training data.

```{r matrix-func-f1}
vector_to_time_series_data <- function(vec, colCount) {
  row_count <- length(vec)
  
  staggered_data_matrix <- matrix(vec, row_count, colCount)
  
  for (i in 1:row_count){
    new_row <- c(staggered_data_matrix[i])
    
    for (j in 1:(colCount-1)){
      new_row <- c(new_row, staggered_data_matrix[i + j])
    }
    
    staggered_data_matrix[i,] <- new_row
  }

  return (as.data.frame(head(staggered_data_matrix, row_count - (colCount -1))))
}
```

With this function we can transform the training currency values now: 
```{r matrix-exchange-f1}
staggered_data_frame <- vector_to_time_series_data(exchange.scaled_train$`USD/EUR`, 4)

colnames(staggered_data_frame) <- c("Input_dneg2", "Input_dneg1", "Input_d0", "Output")

#Summary of training data
head(staggered_data_matrix)
```

##Creating & using the neural net

###Training the netwotk

Now we have the data in a format such that we can provide 3 consecutive days as input and the following day as desired output, we are ready to train the neural network.

```{r train-mlp-f1}
mlp.form1 <- as.formula("Output ~ Input_d0 + Input_dneg1 + Input_dneg2")

mlp.nn1 <- neuralnet(mlp.form1, staggered_data_matrix, hidden = c(8,4,2), threshold=0.01)
```

Now the neural network has been trained we can view a representation of its structure.

```{r display-mlp-f1}
plot(mlp.nn1)
```

###Testing the netwotk

Before we can test the neural network against test data we need to transform the test data we took from the original dataset and transform it so we can pass in three input variables similarly to how the network was trained. Picking the number of inputs and what those inputs might be is part of the challenge of creating a neural network for time-series data; it's pretty obvious that one input value will hardly help project a future value but what value to pick isn't clear. The lower the number or inputs the greater likelihood of error but the higher the number of inputs, the increased cimplexity and longer training period.  I've picked 3 consecutive days to sart with but will experiment with other configuations afterwards.

```{r prep-test-mlp-f1}
staggered_test_data_frame <- vector_to_time_series_data(exchange.scaled_test$`USD/EUR`, 3)
```

Now that the test data is arranged in the same way that the training data is, the neural network can be tested.

```{r testing-mlp-f1}
mlp.nn1_results <- compute(mlp.nn1, staggered_test_data_frame)

test_expected_data.nn1 <- tail(exchange.scaled_test$`USD/EUR`, -3)

test_v_expected.nn1 <- cbind(test_expected_data.nn1, as.data.frame(head(mlp.nn1_results$net.result, -1)))
colnames(test_v_expected.nn1) <- c("Expected Output", "Neural Net Output")
print(test_v_expected.nn1)
```

### Evaluating the predictions

There are two ways we can look at the qualityof the performance of the neural netowrk:
- Various single number values derived from the difference between the predicted and actual values
- Visualising the data, for example, by plotting the estimates and actual values on the sme grapg to more easily draw comparisons by eye.

#### Numeric indicators

And then the Sum of Square Errors, and Mean Squared Error values can be derived in order to look at the performance of the trained nural network against test data. The closer to 0 the values are, the better.

```{r indicator-funcs1-mlp-f1}
output_delta_func <- function (expected, actual) {
  return (c(expected-actual)) 
}
sse_func <- function(x) {
   return (sum( (x - mean(x) )^2 ))
}
mse_func <- function (x) {
  return(sse_func(x)/length(x))
}
```
```{r run-indicator-funcs1-mlp-f1}
test_delta.nn1 <- output_delta_func(test_v_expected.nn1$`Expected Output`, test_v_expected.nn1$`Neural Net Output`)
#SSE of this first nn
sse_func(test_delta.nn1)
#MSE of this first nn
mse_func(test_delta.nn1)
```

Additionally we can look are the performance in terms of the Root Mean Square Error or the Mean Absolute Percentage Error. The Root Mean Square Error is the square root of the average of the squared errors, which allows for the greater individual errors to have a greater influence on the final error value. The Mean Absolute Percentage Error shows the error in terms of the the median of the errors as a fraction of the estimated value, which is displayed as a percentage; the biggest drawback to this measure is the risk of dividing by 0 is the estimated value is 0 but is doesn't allow the biggest deltas to influence the final value more than by their proportions.

What follows are the RMSE and MAPE values for the neural net.

```{r train-mlp-nonl-f1}
#RMSE for the nn
rmse(test_v_expected.nn1$`Neural Net Output`, test_v_expected.nn1$`Expected Output`)
#MAPE for the nn
percent(mape(test_v_expected.nn1$`Neural Net Output`, test_v_expected.nn1$`Expected Output`))
```

#### Visual evaluation

That follows is a graph plotting neural net predicted values against actual values.


```{r train-mlp-nonl-f1}
# Create Line Chart

# convert factor to numeric for convenience 
#Orange$Tree <- as.numeric(Orange$Tree) 
#ntrees <- max(Orange$Tree)
count.col <- ncol(test_v_expected.nn1)
count.row <- nrow(test_v_expected.nn1)
exchange_range <- range(test_v_expected.nn1)

# get the range for the x and y axis 
plot_range.y <- range(test_v_expected.nn1)
plot_range.x <- 1:nrow(test_v_expected.nn1)

# set up the plot 
plot(test_v_expected.nn1$`Expected Output`, type="n", xlab="Days",
  	ylab="Normalised Exchange Rate", ylim=plot_range.y ) 

colors <- rainbow(count.col) 
linetype <- c(1:count.col) 
plotchar <- seq(18,18+count.col,1)

# add lines 
for (i in 1:count.col) { 
  #tree <- subset(Orange, Tree==i) 
  lines(test_v_expected.nn1[i], type="b", lwd=1.5,
    lty=linetype[i], col=colors[i], pch=plotchar[i]) 
} 

# add a title and subtitle 
title("Exchange Rate Prediction Versus Actual", "Comparing actual values for USD/EUR rates against values derived from a Neural Network")

# add a legend 
#legend(test_v_expected.nn1[1], 1:count.col, cex=0.8, col=colors,
#  	pch=plotchar, lty=linetype, title="Exchange Rates")
```

## Experimenting with other Neural Netowrk configucations

The first configuration to experiment with in the `neuralnet` package should be the `linear.output` value which is true by default. It occurs to me that currency rates don't change in a linear fashion so we should create a neural network that reflects this.

```{r train-mlp-nonl-f1}
#mlp.form1 <- as.formula("Output ~ Input_d0 + Input_dneg1 + Input_dneg2")

mlp.nn1_nonl <- neuralnet(mlp.form1, staggered_data_matrix, hidden = c(8,4,2), threshold=0.01, linear.output=FALSE)
```
```{r testing-mlp-nonl-f1}
mlp.nn1_nonl_results <- compute(mlp.nn1_nonl, staggered_test_data_frame)

test_expected_data.nn1_nonl <- tail(exchange.scaled_test$`USD/EUR`, -3)

test_v_expected.nn1_nonl <- cbind(test_expected_data, as.data.frame(head(mlp.nn1_results$net.result, -1)))
colnames(test_v_expected.nn1_nonl) <- c("Expected Output", "Neural Net Output")
print(test_v_expected.nn1_nonl)
```
```{r run-indicator-funcs1-mlp-nonl-f1}
test_delta.nn1_nonl <- output_delta_func(test_v_expected.nn1_nonl$`Expected Output`, test_v_expected.nn1_nonl$`Neural Net Output`)
#SSE of this first nn
sse_func(test_delta.nn1_nonl)
#MSE of this first nn
mse_func(test_delta.nn1_nonl)
```

## Conclusion

...


</section>
